
# Borges
## Part 1: Bowl vase classifier (CNN model)
## Part 2: New shape generation with GAN
This repo consists of two parts. One part is to build a model to classify bowl and vase images; and the other part is to build a model to generate new shapes.

## Part 1: bowl vase classifier 
### Overview
A simple approach would be to use the shape feature as representation of bowls and vases, and train a CNN model. Considering the data is not 'big' (1000 in each class). Overfitting is a concern to address.

[//]:# (Image references)
[bowl_vase]:./output_images/bowl_vase.png
[edge_detection]:./output_images/preprocessed_image.png
[performance]: ./output_images/model_prediction.png
[API_gen_images]: ./output_images/API_samples.png
[GAN_gen_images]: ./output_images/GAN_samples.png


### 1 data 
The data, generated by Borges geometry server API, contains [2000 3d shapes of bowls and vases](https://goo.gl/zz6wYB)

### 2 Data exploration
The image is RGBA format with a size (256,256,4). However, the image has rather large empty margins, especially bowl images. In training, the bowl images are trimmed a bit to reduce unnecessary computation. A sample bowl and vase is shown here:
![][bowl_vase]
### 3 Preprocessing and Feature engineering
The shapes of bowls and vases are quite different and therefore, the shape could be a good representative feature. To get the shape feature, sobel algorithm is used to detect the edges. The image is also cropped (bowls) and resized. A sample is shown here:
![][edge_detection]
### 4 Model
A simple CNN model was built consisting of 1 convolution layer, one pooling layer, and fully connected layers. 

### 5 Performance
The model yielded an accuracy of 100% for training and testing data. However, for downloaded images from online, the model reached only 80% accuracy. The performance on 10 downloaded images is shown below. For each image, the title is the true class, x axis is the probability, and y axis is the predicted class. 8 samples out of 10 are correctly predicted, and interestingly, all vases are correctly predicted.
![][performance]
### 6 Further improvements
This is the first version model and many things can be done from here. 
* First, data augmentation may yield a better result with techniques like brightness distortion, random noise, and simple image flipping. 
* Second, feature engineering is critical, so other features like HOG (Histogram of Oriented Gradients) might also be a good solution. 
* Third, data scaling can help. The first model, I chose simply to scale the data to (0,1), but through some iteration, better scaling method might help, like standardscaler in scikit-learn
* Fourth, transfer learning might be a good idea. Models like VGG, Inception, resNet are good choices to play with
* Fifth, classic machine learning model might also do the work. HOG feature combined with SVM or decision tree could be a good candidate to try
* Sixth, model ensemble is a great technique. Widely used in Kaggle competition, model ensemble seems to be a really good idea to improve a model
* Seventh, cross validation is indispenable for tracking the true performance instead of getting an illusive performance. 
* Eighth, make the model reusable

I will keep improving the model with above points

## Part 2: generative model for new shapes

### Overview
Yann Lecun, FAIR director, once said, Generative Neural Network (and variations) is the most interesting idea in the last ten years in machine learning [quora link](https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning). Recently, Nvidia proposed a new GAN model for ["Progressive Growing of GANs for Improved Quality, Stability, and Variation:](http://research.nvidia.com/publication/2017-10_Progressive-Growing-of). The model is capable of creating HD images progressively. More interestingly, the model can manipulate the latent space to correlate z space to an image, allowing the editing of an image. 

This project consists of three steps. Step 1 is to build a GAN that can generate low resolution images (bowls, more later). Step 2 is to build an auto encoder that can correate z space with a real image (reverse Discriminator). Step 3 is to build an advanced GAN that can generate HD images and allow the images to be modified intuitively. 

### 1 Data
The 1000 Bowl images are certainly not enough to train a GAN. Therefore, I use Borges image generation API to generate more image data (around 18,000 bowl images as of this writing, special thanks to Jochem for showing me how the API works). I wrote two scripts here. One is to generate a new BOM file and the other is to generate a new image with the new BOM file. Both can be found in this repo

### 2 Data preprocess
The images are converted from RGBA to Gray scale and resized to (64, 64, 1) to feed the network

### 3 Model
The GAN model consists of two networks, one generator(G) and one discriminator(D). Some details here: I use batch normalization heavily for almost every layer (not the final layer of D or the first layer of G). Tanh is used as activation function for D. I followed quite some suggestions from [this wonderful GAN hacks](https://github.com/soumith/ganhacks). Noteably, the G is twice as large as D to prevent D from being too strong.

### 4 Training
The model was too large for my macbook pro; therefore I performed the training on an EC2 instance(g2.2x with GPU). 

### 5 Performance
After 10 epochs of training (maybe more data less epochs are better), the mode is able to generate quite nice bowl images. Below are 25 GAN generated images:
![GAN gen images][GAN_gen_images]
as comparison, also 25 API generated images:
![API gen images][API_gen_images]

### 6 Further improvements
As this is step 1 of the whole project, a lot more can be done later. But for this specific step, many improvements can be used. 
- First is the data. 
I only had a loose sense of the BOM file, so I just simply altered the editable values (x:, y:) with a random multiplier to generate a new BOM file. The scripts worked, but clearly can be better. 
- Second is the model. 
GANs are known being trickly to train, so many detailed tricks can be used to improve the performance. 
- Third is the complexity of the model. 
Keras is an amazing tool, especially for prototyping. But I used Tensorflow at the moment, which took longer to build the model(good practice for me :-) ). 
