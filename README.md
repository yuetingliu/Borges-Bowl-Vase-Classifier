
## Bowl vase classifier
Build a model to classify bowl and vase images

### Overview
A simple approach would be to use the shape feature as representation of bowls and vases, and train a CNN model. Considering the data is not 'big' (1000 in each class). Overfitting is a concern to address.

[//]:# (Image references)
[bowl_vase]:./output_images/bowl_vase.png
[edge_detection]:./output_images/preprocessed_image.png
[performance]: ./output_images/model_prediction.png


### 1 data 
The data, generated by Borges geometry server API, contains [2000 3d shapes of bowls and vases](https://goo.gl/zz6wYB)

### 2 Data exploration
The image is RGBA format with a size 256*256*4. However, the image has rather large empty margins, especially bowl images. In training, the bowl images are trimmed a bit to reduce unnecessary computation. A sample bowl and vase is shown here:
![][bowl_vase]

### 3 Feature engineering
The shape of bowls and vases are quite different and therefore, the shape could be a good representative feature. To get the shape feature, sobel algorithm is used to detect the edges A sample is shown here:
![][edge_detection]

### 4 Model
A simple CNN model was built consisting of 1 convolution layer, one pooling layer, and fully connected layers. 

### 5 Performance
The model yielded an accuracy of 100% for training and testing data. However, for downloaded images from online, the model reached only 80% accuracy. The performance on 10 downloaded images is shown below. For each image, the title is the true class, x axis is the probability, and y axis is the predicted class. 8 samples out of 10 are correctly predicted, and interestingly, all vases are correctly predicted.

![][performance]


### 6 Further improvement
This is the first version model and many things can be done from here. 
* First, data augmentation may yield a better result with techniques like brightness distortion, random noise, and simple image flipping. 
* Second, feature engineering is critical, so other features like HOG (Histogram of Oriented Gradients) might also be a good solution. 
* Third, data scaling can help. The first model, I chose simply to scale the data to (0,1), but through some iteration, better scaling method might help, like standardscaler in scikit-learn
* Fourth, transfer learning might be a good idea. Models like VGG, Inception, resNet are good choices to play with
* Fifth, classic machine learning model might also do the work. HOG feature combined with SVM or decision tree could be a good candidate to try
* Sixth, model ensemble is a great technique. Widely used in Kaggle competition, model ensemble seems to be a really good idea to improve a model
* Seventh, cross validation is indispenable for tracking the true performance instead of getting an illusive performance. 
* Eighth, make the model reusable

I will keep improving the model with above points




```python

```
